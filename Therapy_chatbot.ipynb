{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rR00rxd-jPKM"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "import pandas as pd\n",
        "import json\n",
        "import wandb\n",
        "from datasets import load_dataset\n",
        "from transformers import (\n",
        "    T5ForConditionalGeneration, T5Tokenizer,\n",
        "    GPT2LMHeadModel, GPT2Tokenizer,\n",
        "    BartForConditionalGeneration, BartTokenizer,\n",
        "    Trainer, TrainingArguments\n",
        ")\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch\n",
        "\n",
        "# Check for GPU availability\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Initialize WandB and log in with the API key\n",
        "wandb.login(key=\"c58ae33b91d55a794e239c1d62a67ab9eb7ee4a8\")\n",
        "\n",
        "# Define custom tokens for labeling stages\n",
        "special_tokens_dict = {'additional_special_tokens': ['<Exploration>', '<Comforting>', '<Action>']}\n",
        "\n",
        "# Load the ESConv dataset and label stages\n",
        "ds = load_dataset(\"thu-coai/esconv\")\n",
        "\n",
        "# Define strategy-to-stage mapping for the ESConv dataset\n",
        "strategy_to_stage = {\n",
        "    \"Question\": \"Exploration\",\n",
        "    \"Affirmation and Reassurance\": \"Comforting\",\n",
        "    \"Self-disclosure\": \"Comforting\",\n",
        "    \"Providing Suggestions\": \"Action\",\n",
        "    \"Restatement or Paraphrasing\": \"Exploration\",\n",
        "    \"Reflection of feelings\": \"Comforting\",\n",
        "    \"Others\": \"Exploration\"  # Default to Exploration if unsure\n",
        "}\n",
        "\n",
        "# Process the ESConv dataset to extract client-therapist pairs and assign stages\n",
        "client_texts, therapist_texts, stages = [], [], []\n",
        "for entry in ds['train']:\n",
        "    content = json.loads(entry['text'])\n",
        "    if 'dialog' in content:\n",
        "        client_message = None\n",
        "        for message in content['dialog']:\n",
        "            if message['speaker'] == \"usr\":\n",
        "                client_message = message['text']\n",
        "            elif message['speaker'] == \"sys\" and client_message:\n",
        "                strategy = message.get(\"strategy\", \"Others\")\n",
        "                stage = strategy_to_stage.get(strategy, \"Exploration\")\n",
        "                client_texts.append(client_message)\n",
        "                therapist_texts.append(message['text'])\n",
        "                stages.append(stage)\n",
        "                client_message = None  # Reset client message after pairing\n",
        "\n",
        "# Create a DataFrame to structure the dataset\n",
        "df = pd.DataFrame({\"Client\": client_texts, \"Therapist\": therapist_texts, \"Stage\": stages})\n",
        "df['Client'] = df['Client'].fillna('').astype(str)\n",
        "df['Therapist'] = df['Therapist'].fillna('').astype(str)\n",
        "df['Stage'] = df['Stage'].fillna('').astype(str)\n",
        "\n",
        "# Convert data to lists for training\n",
        "client_texts = df['Client'].tolist()\n",
        "therapist_texts = df['Therapist'].tolist()\n",
        "stage_texts = df['Stage'].tolist()\n",
        "\n",
        "# Initialize and customize tokenizers with custom tokens\n",
        "t5_tokenizer = T5Tokenizer.from_pretrained(\"t5-small\")\n",
        "t5_tokenizer.add_special_tokens(special_tokens_dict)\n",
        "t5_tokenizer.pad_token = t5_tokenizer.eos_token\n",
        "\n",
        "gpt_tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
        "gpt_tokenizer.add_special_tokens(special_tokens_dict)\n",
        "gpt_tokenizer.pad_token = gpt_tokenizer.eos_token\n",
        "\n",
        "bart_tokenizer = BartTokenizer.from_pretrained(\"facebook/bart-base\")\n",
        "bart_tokenizer.add_special_tokens(special_tokens_dict)\n",
        "bart_tokenizer.pad_token = bart_tokenizer.eos_token\n",
        "\n",
        "# Initialize and customize models with resized embeddings\n",
        "t5_model = T5ForConditionalGeneration.from_pretrained(\"t5-small\").to(device)\n",
        "t5_model.resize_token_embeddings(len(t5_tokenizer))\n",
        "\n",
        "gpt_model = GPT2LMHeadModel.from_pretrained(\"gpt2\").to(device)\n",
        "gpt_model.resize_token_embeddings(len(gpt_tokenizer))\n",
        "\n",
        "bart_model = BartForConditionalGeneration.from_pretrained(\"facebook/bart-base\").to(device)\n",
        "bart_model.resize_token_embeddings(len(bart_tokenizer))\n",
        "\n",
        "# Custom Dataset class with sharding for memory-efficient loading\n",
        "class ShardedConversationDataset(Dataset):\n",
        "    def __init__(self, client_texts, therapist_texts, stage_texts, tokenizer, shard_size=10000, max_length=512):\n",
        "        self.client_texts = client_texts\n",
        "        self.therapist_texts = therapist_texts\n",
        "        self.stage_texts = stage_texts\n",
        "        self.tokenizer = tokenizer\n",
        "        self.shard_size = shard_size\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def load_shard(self, shard_index):\n",
        "        # Calculate start and end indices for the shard\n",
        "        self.start_idx = shard_index * self.shard_size\n",
        "        end_idx = min((shard_index + 1) * self.shard_size, len(self.client_texts))\n",
        "\n",
        "        # Combine Stage and Client text\n",
        "        inputs = [\n",
        "            f\"Stage: {stage} | Client: {client}\"\n",
        "            for stage, client in zip(self.stage_texts[self.start_idx:end_idx], self.client_texts[self.start_idx:end_idx])\n",
        "        ]\n",
        "\n",
        "        # Tokenize inputs and therapist responses\n",
        "        self.client_tokens = self.tokenizer(inputs, padding='max_length', truncation=True, max_length=self.max_length, return_tensors='pt')\n",
        "        self.therapist_tokens = self.tokenizer(self.therapist_texts[self.start_idx:end_idx], padding='max_length', truncation=True, max_length=self.max_length, return_tensors='pt')\n",
        "\n",
        "        # Set labels and mask padding tokens\n",
        "        self.therapist_tokens['labels'] = self.therapist_tokens['input_ids'].clone()\n",
        "        self.therapist_tokens['labels'][self.therapist_tokens['input_ids'] == self.tokenizer.pad_token_id] = -100\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.client_tokens['input_ids'])\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Do not move data to device here\n",
        "        return {\n",
        "            'input_ids': self.client_tokens['input_ids'][idx],\n",
        "            'attention_mask': self.client_tokens['attention_mask'][idx],\n",
        "            'labels': self.therapist_tokens['labels'][idx]\n",
        "        }\n",
        "\n",
        "# Train each model using customized tokenizer and model with TrainingArguments\n",
        "shard_size = 10000\n",
        "models_and_tokenizers = [\n",
        "    {\"model\": t5_model, \"tokenizer\": t5_tokenizer, \"output_dir\": \"./results/t5_small\"},\n",
        "    {\"model\": gpt_model, \"tokenizer\": gpt_tokenizer, \"output_dir\": \"./results/gpt2\"},\n",
        "    {\"model\": bart_model, \"tokenizer\": bart_tokenizer, \"output_dir\": \"./results/bart_base\"}\n",
        "]\n",
        "\n",
        "# Training loop for each model\n",
        "for setup in models_and_tokenizers:\n",
        "    model = setup[\"model\"]\n",
        "    tokenizer = setup[\"tokenizer\"]\n",
        "    output_dir = setup[\"output_dir\"]\n",
        "\n",
        "    # Initialize the sharded dataset with the model's tokenizer\n",
        "    sharded_dataset = ShardedConversationDataset(client_texts, therapist_texts, stage_texts, tokenizer, shard_size=shard_size)\n",
        "\n",
        "    # Determine the number of shards\n",
        "    total_shards = len(client_texts) // shard_size + (len(client_texts) % shard_size != 0)\n",
        "    for shard_index in range(total_shards):\n",
        "        print(f\"Loading shard {shard_index + 1}/{total_shards} for model {model.__class__.__name__}\")\n",
        "\n",
        "        sharded_dataset.load_shard(shard_index)\n",
        "        dataloader = DataLoader(sharded_dataset, batch_size=8, shuffle=True)\n",
        "\n",
        "        # Define training arguments with wandb logging\n",
        "        training_args = TrainingArguments(\n",
        "            output_dir=output_dir,\n",
        "            evaluation_strategy=\"steps\",\n",
        "            per_device_train_batch_size=8,\n",
        "            per_device_eval_batch_size=8,\n",
        "            num_train_epochs=5,\n",
        "            save_steps=1000,\n",
        "            save_total_limit=2,\n",
        "            logging_dir='./logs',\n",
        "            report_to=\"wandb\"  # Log to WandB\n",
        "        )\n",
        "\n",
        "        # Initialize the Trainer\n",
        "        trainer = Trainer(\n",
        "            model=model,\n",
        "            args=training_args,\n",
        "            train_dataset=sharded_dataset,\n",
        "            eval_dataset=sharded_dataset\n",
        "        )\n",
        "\n",
        "        # Train on the current shard\n",
        "        trainer.train()\n",
        "        model.save_pretrained(f\"{output_dir}shard{shard_index + 1}\")\n",
        "\n",
        "    # Save the final model and tokenizer\n",
        "    model.save_pretrained(f\"{output_dir}_final\")\n",
        "    tokenizer.save_pretrained(f\"{output_dir}_tokenizer\")"
      ]
    }
  ]
}